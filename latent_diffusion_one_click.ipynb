{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latent-diffusion-one-click.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xEVSOJ4f0B21"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Diffusion One-Click Notebook\n",
        "\n",
        "Based on [this notebook](https://colab.research.google.com/github/multimodalart/latent-diffusion-notebook/blob/main/Latent_Diffusion_LAION_400M_model_text_to_image.ipynb#scrollTo=fmafGmcyT1mZ)\n",
        "by [multimodalart](https://github.com/multimodalart),\n",
        "with edits by [mdnestor](https://github.com/mdnestor)."
      ],
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "model_path = \"/models/ldm/text2img-large\"\n",
        "outputs_path = \"/content/outputs\"\n",
        "!mkdir -p $model_path\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Model will be stored at {model_path}\")\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n",
        "\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "!pip install open_clip_torch\n",
        "!pip install autokeras\n",
        "!pip install tensorflow\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan \n",
        "\n",
        "%cd /content/latent-diffusion\n",
        "\n",
        "import os\n",
        "if os.path.isfile(f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\"):\n",
        "    print(\"Using saved model from Google Drive\")\n",
        "else:    \n",
        "    !wget -O $model_path/latent_diffusion_txt2img_f8_large.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n",
        "\n",
        "\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "tqdm_auto_model = __import__(\"tqdm.auto\", fromlist=[None]) \n",
        "sys.modules['tqdm'] = tqdm_auto_model\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from open_clip import tokenizer\n",
        "import open_clip\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def load_safety_model(clip_model):\n",
        "    \"\"\"load the safety model\"\"\"\n",
        "    import autokeras as ak  # pylint: disable=import-outside-toplevel\n",
        "    from tensorflow.keras.models import load_model  # pylint: disable=import-outside-toplevel\n",
        "    from os.path import expanduser  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "    home = expanduser(\"~\")\n",
        "\n",
        "    cache_folder = home + \"/.cache/clip_retrieval/\" + clip_model.replace(\"/\", \"_\")\n",
        "    if clip_model == \"ViT-L/14\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_binary_nsfw\"\n",
        "        dim = 768\n",
        "    elif clip_model == \"ViT-B/32\":\n",
        "        model_dir = cache_folder + \"/clip_autokeras_nsfw_b32\"\n",
        "        dim = 512\n",
        "    else:\n",
        "        raise ValueError(\"Unknown clip model\")\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(cache_folder, exist_ok=True)\n",
        "\n",
        "        from urllib.request import urlretrieve  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        path_to_zip_file = cache_folder + \"/clip_autokeras_binary_nsfw.zip\"\n",
        "        if clip_model == \"ViT-L/14\":\n",
        "            url_model = \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_binary_nsfw.zip\"\n",
        "        elif clip_model == \"ViT-B/32\":\n",
        "            url_model = (\n",
        "                \"https://raw.githubusercontent.com/LAION-AI/CLIP-based-NSFW-Detector/main/clip_autokeras_nsfw_b32.zip\"\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model {}\".format(clip_model))\n",
        "        urlretrieve(url_model, path_to_zip_file)\n",
        "        import zipfile  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "        with zipfile.ZipFile(path_to_zip_file, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(cache_folder)\n",
        "\n",
        "    loaded_model = load_model(model_dir, custom_objects=ak.CUSTOM_OBJECTS)\n",
        "    loaded_model.predict(np.random.rand(10 ** 3, dim).astype(\"float32\"), batch_size=10 ** 3)\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "def is_unsafe(safety_model, embeddings, threshold=0.5):\n",
        "    \"\"\"find unsafe embeddings\"\"\"\n",
        "    nsfw_values = safety_model.predict(embeddings, batch_size=embeddings.shape[0])\n",
        "    x = np.array([e[0] for e in nsfw_values])\n",
        "    #print(x)\n",
        "    return True if x > threshold else False\n",
        "#NSFW CLIP Filter\n",
        "safety_model = load_safety_model(\"ViT-B/32\")\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\") \n",
        "model = load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if opt.plms:\n",
        "        opt.ddim_eta = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    prompt = opt.prompt\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with model.ema_scope():\n",
        "                uc = None\n",
        "                if opt.scale > 0:\n",
        "                    uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                    shape = [4, opt.H//8, opt.W//8]\n",
        "                    samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                    conditioning=c,\n",
        "                                                    batch_size=opt.n_samples,\n",
        "                                                    shape=shape,\n",
        "                                                    verbose=False,\n",
        "                                                    unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,\n",
        "                                                    eta=opt.ddim_eta)\n",
        "\n",
        "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    for x_sample in x_samples_ddim:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        image_vector = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        image = preprocess(image_vector).unsqueeze(0)\n",
        "                        with torch.no_grad():\n",
        "                          image_features = clip_model.encode_image(image)\n",
        "                        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                        query = image_features.cpu().detach().numpy().astype(\"float32\")\n",
        "                        unsafe = is_unsafe(safety_model,query,opt.nsfw_threshold)\n",
        "                        if(not unsafe):\n",
        "                          image_vector.save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                        else:\n",
        "                          raise Exception('Potential NSFW content was detected on your outputs. Try again with different prompts. If you feel your prompt was not supposed to give NSFW outputs, this may be due to a bias in the model')\n",
        "                        base_count += 1\n",
        "                    all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "    display(Image.fromarray(grid.astype(np.uint8)))\n",
        "    #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")\n",
        "\n",
        "import argparse"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aJF6wP2zkWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = argparse.Namespace(\n",
        "  prompt = \"moses and the burning bush funko pop\", \n",
        "  n_iter = 3, n_samples = 3, W = 128, H = 128,\n",
        "  ddim_steps = 50, ddim_eta = 0.0, scale = 5.0,\n",
        "  plms = False, nsfw_threshold = 1.0, outdir = '.',\n",
        ")\n",
        "run(args)"
      ],
      "metadata": {
        "id": "fmafGmcyT1mZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}